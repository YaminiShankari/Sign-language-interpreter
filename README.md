# ğŸ–ï¸ Sign Language Interpreter

This project uses Computer Vision and Deep Learning to recognize basic hand gestures corresponding to common sign language words â€” all in real time through your webcam!

It captures gesture images, trains a custom CNN (Convolutional Neural Network) model using TensorFlow, and interprets live gestures into words like â€œhelloâ€, â€œthanksâ€, â€œyesâ€, and â€œnoâ€.

# ğŸ“‚ Project Files Overview

BACKEND FILES

1ï¸âƒ£ dataset_collection.py

ğŸ¥ Purpose: Collects gesture images for each sign label.

-Opens your webcam and captures images for defined labels (hello, thanks, yes, no).   
-Stores them in a structured dataset directory.   
-Automatically creates subfolders and saves around 35 images per label.   

ğŸ§  Tip: Keep consistent lighting and background while recording your gestures.

2ï¸âƒ£ labelImg.exe

ğŸ–Šï¸ Purpose: Annotate gesture images for model training.

-After capturing your gesture images using dataset_collection.py, use LabelImg to draw bounding boxes around your hand and assign the correct label (like hello, thanks, yes, no).   
-This generates .xml files in Pascal VOC format for each image.   

ğŸ“ Steps to use:

Open labelImg.exe   
Load your captured image folder   
Draw a box around your hand region   
Assign the appropriate label name    
Save â†’ generates image_name.xml   

âš¡ Why it matters:
These XML annotations are crucial for linking each image to its correct class during training â€” your train.py reads them to pair the image data with its label.

3ï¸âƒ£ train.py

ğŸ§© Purpose: Trains a CNN model for gesture classification.

-Loads gesture images and corresponding XML annotation files.   
-Preprocesses images (resizing, normalization, one-hot encoding).   
-Builds a 3-layer CNN model with ReLU activations and softmax output.   
-Trains and validates the model using an 80/20 split.   
-Saves the trained model as sign_language_model.keras.   

âš™ï¸ Model Architecture Overview:

Conv2D â†’ MaxPooling2D (x3 layers)   
Flatten â†’ Dense (128 neurons)   
Output Layer: 4 neurons (softmax for 4 gesture classes)   

4ï¸âƒ£ final.py

ğŸ‘ï¸ Purpose: Real-time gesture recognition and prediction.

-Loads the trained model.   
-Detects your hand using HSV color filtering (skin segmentation).   
-Extracts the hand region, preprocesses it, and predicts the gesture.   
-Displays real-time predictions with bounding boxes and labels.   

5ï¸âƒ£ server.py

ğŸŒ Purpose: Serves the backend API for the frontend.

-Runs a Flask server with CORS enabled.   
-Accepts base64-encoded images from the frontend.   
-Returns predicted gesture and confidence.   

FRONTEND FILES

The frontend is built with React, using Webcam for live capture and Axios to communicate with the Flask backend.

1ï¸âƒ£ App.jsx

ğŸ¨ Purpose: Handles the user interface and live webcam feed.

-Switches between home page and interpreter page.   
-Captures webcam frames every second and sends to backend API.   
-Displays predicted gesture and confidence in real-time.   
-Includes animations via framer-motion and responsive design with Tailwind CSS.   

2ï¸âƒ£ Dependencies

Ensure these packages are installed for the frontend:   
npm install react react-dom react-scripts axios react-webcam framer-motion tailwindcss

3ï¸âƒ£ Starting the Frontend   

cd frontend   
npm install   
npm run dev   

# ğŸ’¡ Future Enhancements

-Add more gestures (like sorry, please)   
-Integrate with a voice output system   
-Improve accuracy with MediaPipe hand landmarks   

# ğŸ Results

âœ… Accurate real-time detection   
âœ… Smooth performance with webcam feed   
âœ… Easy dataset expansion and retraining   

# âœ¨ Credits

Developer: Yamini Shankari AJ ğŸ‘©â€ğŸ’»
Tech Stack: Python, OpenCV, TensorFlow, Keras, React, Tailwind CSS
â€œBreaking barriers through gestures â€” one sign at a time.â€ ğŸŒ
